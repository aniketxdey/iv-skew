{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9l9kX1XexbS"
      },
      "source": [
        "# **Comparing Implied Volatility and Market Downturns using BS & Logistic Regression**\n",
        "\n",
        "\n",
        "\n",
        "## **Introduction**:\n",
        "\n",
        "* Research Question: What factors contribute to the presence and magnitude of the volatility smirk in equity options, and how do their dynamics correlate\n",
        "with events and annualized volatility across different market sectors (ie: tech - high volatility vs. utilities - low volatility)? Specifically, do abrupt changes, like a sudden steepening in the IV smirk, reliably forecast short-term market declines or volatility spikes beyond standard risk measures (VIX)?\n",
        "\n",
        "We hypothesize that abrupt changes in the implied volatility smirk, characterized as significant increases between the out-of-the-money put & the at-the-money call implied volatilities, predict a higher probability of short-term market downturns. Furthermore, the shift captures the investor sentiment and jump risk not fully reflected by historical volatility or the VIX, thereby offering superior early-warning capabilities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ-GdRjZfk3q"
      },
      "source": [
        "# **Data & Pre-Processing**\n",
        "\n",
        "### Dataset Overview\n",
        "This analysis uses **daily put and call options data** for the Invesco QQQ ETF (Nasdaq: QQQ) from Q1 2020 to Q4 2022. The QQQ tracks the Nasdaq-100 Index, which is heavily weighted toward technology stocks. This period was chosen due to its extreme volatility, including:\n",
        "- The COVID-19 market crash (Q1 2020)\n",
        "- The tech-driven recovery (2021)\n",
        "- The 2022 bear market driven by rising interest rates.\n",
        "\n",
        "The dataset includes:\n",
        "- **Option metrics**: Implied volatility (IV), delta, bid/ask prices, volume, and strike details.\n",
        "- **Underlying asset data**: Daily closing prices for QQQ.\n",
        "- **Time-to-expiration**: Days until contract expiry (`DTE`).\n",
        "\n",
        "### Preprocessing Steps\n",
        "1. **Column Standardization**:  \n",
        "   Column names were cleaned (whitespace removed) and renamed for consistency.  \n",
        "   Example: `[QUOTE_DATE]` → `QUOTE_DATE`.\n",
        "\n",
        "2. **Data Type Conversion**:  \n",
        "   - Dates (`QUOTE_DATE`, `EXPIRE_DATE`) converted to `datetime`.  \n",
        "   - Numeric fields (e.g., `C_BID`, `P_IV`) coerced to floats, handling invalid entries.  \n",
        "\n",
        "3. **Time-to-Expiration Calculation**:  \n",
        "   Computed as:  \n",
        "   $$\n",
        "   \\text{DTE} = \\text{EXPIRE_DATE} - \\text{QUOTE_DATE}\n",
        "   $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9hzO5E80IQt",
        "outputId": "656d1d16-074e-48c8-a18e-5f2c6bcf96b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-058d80be8275>:15: DtypeWarning: Columns (15,17,18,20,21,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(filepath)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All required columns present: ['QUOTE_UNIXTIME', 'QUOTE_READTIME', 'QUOTE_DATE', 'QUOTE_TIME_HOURS', 'UNDERLYING_LAST', 'EXPIRE_DATE', 'EXPIRE_UNIX', 'DTE', 'C_DELTA', 'C_GAMMA', 'C_VEGA', 'C_THETA', 'C_RHO', 'C_IV', 'C_VOLUME', 'C_LAST', 'C_SIZE', 'C_BID', 'C_ASK', 'STRIKE', 'P_BID', 'P_ASK', 'P_SIZE', 'P_LAST', 'P_DELTA', 'P_GAMMA', 'P_VEGA', 'P_THETA', 'P_RHO', 'P_IV', 'P_VOLUME', 'STRIKE_DISTANCE', 'STRIKE_DISTANCE_PCT']\n",
            "0            4\n",
            "1            4\n",
            "2            4\n",
            "3            4\n",
            "4            4\n",
            "          ... \n",
            "1775744    836\n",
            "1775745    836\n",
            "1775746    836\n",
            "1775747    836\n",
            "1775748    836\n",
            "Name: DTE, Length: 1775749, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Mount Google Drive and load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "filepath = '/content/drive/MyDrive/Colab Notebooks/qqq_2020_2022.csv'\n",
        "df = pd.read_csv(filepath)\n",
        "\n",
        "# Data Cleaning and Preparation\n",
        "# First clean all column names by stripping whitespace\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Create comprehensive rename dictionary for ALL columns\n",
        "full_rename_dict = {\n",
        "    '[QUOTE_UNIXTIME]': 'QUOTE_UNIXTIME',\n",
        "    '[QUOTE_READTIME]': 'QUOTE_READTIME',\n",
        "    '[QUOTE_DATE]': 'QUOTE_DATE',\n",
        "    '[QUOTE_TIME_HOURS]': 'QUOTE_TIME_HOURS',\n",
        "    '[UNDERLYING_LAST]': 'UNDERLYING_LAST',\n",
        "    '[EXPIRE_DATE]': 'EXPIRE_DATE',\n",
        "    '[EXPIRE_UNIX]': 'EXPIRE_UNIX',\n",
        "    '[DTE]': 'DTE',\n",
        "    '[C_DELTA]': 'C_DELTA',\n",
        "    '[C_GAMMA]': 'C_GAMMA',\n",
        "    '[C_VEGA]': 'C_VEGA',\n",
        "    '[C_THETA]': 'C_THETA',\n",
        "    '[C_RHO]': 'C_RHO',\n",
        "    '[C_IV]': 'C_IV',\n",
        "    '[C_VOLUME]': 'C_VOLUME',\n",
        "    '[C_LAST]': 'C_LAST',\n",
        "    '[C_SIZE]': 'C_SIZE',\n",
        "    '[C_BID]': 'C_BID',\n",
        "    '[C_ASK]': 'C_ASK',\n",
        "    '[STRIKE]': 'STRIKE',\n",
        "    '[P_BID]': 'P_BID',\n",
        "    '[P_ASK]': 'P_ASK',\n",
        "    '[P_SIZE]': 'P_SIZE',\n",
        "    '[P_LAST]': 'P_LAST',\n",
        "    '[P_DELTA]': 'P_DELTA',\n",
        "    '[P_GAMMA]': 'P_GAMMA',\n",
        "    '[P_VEGA]': 'P_VEGA',\n",
        "    '[P_THETA]': 'P_THETA',\n",
        "    '[P_RHO]': 'P_RHO',\n",
        "    '[P_IV]': 'P_IV',\n",
        "    '[P_VOLUME]': 'P_VOLUME',\n",
        "    '[STRIKE_DISTANCE]': 'STRIKE_DISTANCE',\n",
        "    '[STRIKE_DISTANCE_PCT]': 'STRIKE_DISTANCE_PCT'\n",
        "}\n",
        "\n",
        "# Apply rename in one operation\n",
        "df = df.rename(columns=full_rename_dict)\n",
        "\n",
        "# Verify columns exist\n",
        "assert 'STRIKE_DISTANCE_PCT' in df.columns\n",
        "print(\"All required columns present:\", list(df.columns))\n",
        "\n",
        "# Convert and calculate necessary fields\n",
        "df['QUOTE_DATE'] = pd.to_datetime(df['QUOTE_DATE'])\n",
        "df['EXPIRE_DATE'] = pd.to_datetime(df['EXPIRE_DATE'])\n",
        "numeric_cols = ['C_BID', 'C_ASK', 'P_BID', 'P_ASK', 'C_IV', 'P_IV', 'C_DELTA', 'P_DELTA']\n",
        "for col in numeric_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "df['DTE'] = (df['EXPIRE_DATE'] - df['QUOTE_DATE']).dt.days\n",
        "print(df['DTE'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjEQ_svFhgnF"
      },
      "source": [
        "# Enhanced Skew Calculation\n",
        "\n",
        "### Methodology\n",
        "We adopt a **delta-based skew decomposition** inspired by Doran & Krieger (2010), extending it to capture the full volatility smile:\n",
        "\n",
        "**Put Skew Measures**:  \n",
        "   - **Deep OTM puts**: $$(\\Delta \\leq -0.25)$$\n",
        "   - **OTM puts**: $$(-0.25 < \\Delta \\leq -0.15)$$\n",
        "   - **ATM puts**: $$(-0.15 < \\Delta \\leq -0.05)$$  \n",
        "\n",
        "   Differences in implied volatility (IV) between these groups:  \n",
        "   $$\n",
        "   \\Delta s_{Pdo,o} = s_{Pdo} - s_{Po} \\quad \\text{(slope of put skew)}\n",
        "   $$\n",
        "   $$\n",
        "   \\Delta s_{Pdo,a} = s_{Pdo} - s_{Pa} \\quad \\text{(curvature of put skew)}\n",
        "   $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvETbOwJ7rLb",
        "outputId": "baeee68c-1afa-4a92-b056-c38740c2fee1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-62651c9188f7>:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  skew_df = df.groupby(['QUOTE_DATE', 'EXPIRE_DATE']).apply(calculate_skew_measures).reset_index()\n"
          ]
        }
      ],
      "source": [
        "# Enhanced Skew Calculation\n",
        "def calculate_skew_measures(group):\n",
        "    put_cond = group['P_DELTA'] < 0\n",
        "    s_Pdo = group[put_cond & (group['P_DELTA'] <= -0.25)]['P_IV'].mean()\n",
        "    s_Po = group[put_cond & (group['P_DELTA'] > -0.25) & (group['P_DELTA'] <= -0.15)]['P_IV'].mean()\n",
        "    s_Pa = group[put_cond & (group['P_DELTA'] > -0.15) & (group['P_DELTA'] <= -0.05)]['P_IV'].mean()\n",
        "\n",
        "    call_cond = group['C_DELTA'] > 0\n",
        "    s_Cdo = group[call_cond & (group['C_DELTA'] >= 0.25)]['C_IV'].mean()\n",
        "    s_Co = group[call_cond & (group['C_DELTA'] >= 0.15) & (group['C_DELTA'] <= 0.25)]['C_IV'].mean()\n",
        "\n",
        "    return pd.Series({\n",
        "        'Δs_Pdo_o': s_Pdo - s_Po,\n",
        "        'Δs_Pdo_a': s_Pdo - s_Pa,\n",
        "        'Δs_Cdo_o': s_Cdo - s_Co,\n",
        "        'ATM_IV': group[(group['STRIKE_DISTANCE_PCT'] <= 0.05)]['P_IV'].mean(),\n",
        "        'BidAsk_Spread': (group['P_ASK'] - group['P_BID']).mean(),\n",
        "        'Volume': group['P_VOLUME'].sum(),\n",
        "        'DTE': group['DTE'].iloc[0]  # Include DTE\n",
        "    })\n",
        "\n",
        "skew_df = df.groupby(['QUOTE_DATE', 'EXPIRE_DATE']).apply(calculate_skew_measures).reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-ECsWPvhs2t"
      },
      "source": [
        "# Market Data Prep & Lee-Mykland Jump Detection\n",
        "\n",
        "### Jump Detection Methodology\n",
        "We use the **Lee-Mykland (2008) nonparametric jump detection framework**, which identifies jumps by standardizing returns against local volatility:\n",
        "\n",
        "1. **Local Volatility Estimate**:  \n",
        "   Rolling 30-day standard deviation:  \n",
        "   $$\n",
        "   \\hat{\\sigma}_t = \\text{std}(\\log(S_{t-29:t}))\n",
        "   $$\n",
        "\n",
        "2. **Test Statistic**:  \n",
        "   $$\n",
        "   T(t) = \\frac{\\log(S_t) - \\log(S_{t-1})}{\\hat{\\sigma}_{t-1}}\n",
        "   $$\n",
        "\n",
        "3. **Jump Threshold**:  \n",
        "   A return is flagged as a jump if:  \n",
        "   $$\n",
        "   T(t) < -3.0 \\quad \\text{(99.7% confidence under normality)}\n",
        "   $$\n",
        "\n",
        "### Code Implementation\n",
        "```python\n",
        "def detect_jumps(returns, window=30, critical=3.0):\n",
        "    local_vol = returns.rolling(window).std()\n",
        "    T_stats = returns / local_vol.shift(1)\n",
        "    return (T_stats < -critical).astype(int)\n",
        "```\n",
        "\n",
        "### Advantages Over Fixed Thresholds\n",
        "- **Adjusts for volatility regimes**: A -3% return in a low-volatility market is treated differently than in a high-volatility market.\n",
        "- **Reduces false positives**: Only extreme moves relative to recent volatility are flagged.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPlYFyII7rHP"
      },
      "outputs": [],
      "source": [
        "# Market Data Preparation\n",
        "market_df = df.groupby('QUOTE_DATE')['UNDERLYING_LAST'].last().reset_index()\n",
        "market_df.columns = ['Date', 'Close']\n",
        "market_df['log_ret'] = np.log(market_df['Close']/market_df['Close'].shift(1))\n",
        "\n",
        "# Lee-Mykland Jump Detection\n",
        "def detect_jumps(returns, window=30, critical=3.0):\n",
        "    local_vol = returns.rolling(window).std()\n",
        "    T_stats = returns / local_vol.shift(1)\n",
        "    return (T_stats < -critical).astype(int)\n",
        "\n",
        "market_df['Jump'] = detect_jumps(market_df['log_ret'])\n",
        "market_df['Date'] = pd.to_datetime(market_df['Date'])\n",
        "\n",
        "# Merge Datasets\n",
        "skew_df['QUOTE_DATE'] = pd.to_datetime(skew_df['QUOTE_DATE'])\n",
        "merged_data = pd.merge_asof(\n",
        "    market_df.sort_values('Date'),\n",
        "    skew_df.sort_values('QUOTE_DATE'),\n",
        "    left_on='Date',\n",
        "    right_on='QUOTE_DATE',\n",
        "    direction='backward'\n",
        ")\n",
        "\n",
        "merged_data['DTE'] = (merged_data['EXPIRE_DATE'] - merged_data['QUOTE_DATE']).dt.days  # Optional safeguard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrHXUenfh5a0"
      },
      "source": [
        "\n",
        "\n",
        "# Feature Engineering & Adjustments\n",
        "\n",
        "### Key Features\n",
        "1. **Maturity Binning**:  \n",
        "   Options grouped into expiration cohorts:  \n",
        "   ```python\n",
        "   merged_data['MaturityBin'] = pd.cut(..., bins=[10, 30, 60, 90])\n",
        "   ```  \n",
        "   Rationale: Skew dynamics differ for near- vs. long-dated options.\n",
        "\n",
        "2. **Controls**:  \n",
        "   - **ATM IV**: Controls for overall market volatility (Bollen & Whaley, 2004).  \n",
        "   - **Bid-Ask Spread**: Proxy for liquidity conditions.  \n",
        "   - **Volume**: Captures speculative activity (Cremers & Weinbaum, 2010).\n",
        "\n",
        "### Class Imbalance Adjustment\n",
        "Jumps are rare events (<5% of observations). We upsample the minority class:  \n",
        "```python\n",
        "minority_upsampled = resample(minority, n_samples=len(majority))\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXuX-LMb7rBd"
      },
      "outputs": [],
      "source": [
        "# Feature Engineering\n",
        "merged_data['MaturityBin'] = pd.cut(merged_data['DTE'],\n",
        "                                   bins=[10, 30, 60, 90],\n",
        "                                   labels=['10-30d', '31-60d', '61-90d'])\n",
        "\n",
        "# Final DataFrame Preparation\n",
        "model_df = merged_data[['Jump', 'Δs_Pdo_o', 'Δs_Pdo_a', 'Δs_Cdo_o',\n",
        "                       'ATM_IV', 'BidAsk_Spread', 'Volume']].dropna()\n",
        "\n",
        "# Handle Class Imbalance\n",
        "majority = model_df[model_df.Jump == 0]\n",
        "minority = model_df[model_df.Jump == 1]\n",
        "minority_upsampled = resample(minority,\n",
        "                             replace=True,\n",
        "                             n_samples=len(majority),\n",
        "                             random_state=42)\n",
        "balanced_df = pd.concat([majority, minority_upsampled])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW2Y2c20iP-N"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "***Note For Regression Analysis: Because of the model we imported in, variable coefficient numbering is reverse sorted in the actual model, and x4 represents 'Δs_Pdo_o'/'Δs_Pdo_a' in regression, while x1 is 'Volume***'\n",
        "\n",
        "\n",
        "### Model Specification\n",
        "#### Regression 1 - Put Skew Slope:\n",
        "$$\n",
        "\\text{Prob(Jump)} = \\Phi\\left( \\beta_0 + \\beta_1 \\Delta s_{Pdo,o}  + \\beta_2 \\text{ATM IV} + \\beta_3 \\text{BidAsk} + \\beta_4 \\text{Volume} \\right)\n",
        "$$\n",
        "#### Regression 2 - Put Skew Curvature:\n",
        "$$\n",
        "\\text{Prob(Jump)} = \\Phi\\left( \\beta_0 + \\beta_1 \\Delta s_{Pdo,a} + \\beta_2 \\text{ATM IV} + \\beta_3 \\text{BidAsk} + \\beta_4 \\text{Volume} \\right)\n",
        "$$\n",
        "\n",
        "### Diagnostics\n",
        "#### Regression 1 - Put Skew Slope:\n",
        "- **Pseudo \\(R^2\\)**: 0.3332 (moderate explanatory power).  \n",
        "\n",
        "#### Regression 2 - Put Skew Curvature:\n",
        "- **Pseudo \\(R^2\\)**: 0.3467 (moderate explanatory power).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "VXXSJAYW75Ix",
        "outputId": "448ffa97-da1f-4561-b353-8d22ac8260ad"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "could not convert string to float: ' 10.000000     50.000000 25.000000  3.000000 1.000000 24.000000 32.000000 5.000000 37.000000 7.000000 44.000000 3.000000 16.000000 14.000000 2.000000 11.000000 5.000000  3.000000 8.000000 111.000000 4.000000 0.000000 53.000000 0.000000  7.000000 8.000000 1.000000 2.000000 2.000000 0.000000 30.000000  13.000000 12.000000 0.000000 0.000000 208.000000 0.000000 0.000000    1.000000     3.000000 2.000000                   1.000000     '",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-14689e2c6545>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mX1_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mX1_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_fit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer_skip_nested_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \"\"\"\n\u001b[1;32m    929\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1053\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m   2151\u001b[0m     ) -> np.ndarray:\n\u001b[1;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2153\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2154\u001b[0m         if (\n\u001b[1;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: ' 10.000000     50.000000 25.000000  3.000000 1.000000 24.000000 32.000000 5.000000 37.000000 7.000000 44.000000 3.000000 16.000000 14.000000 2.000000 11.000000 5.000000  3.000000 8.000000 111.000000 4.000000 0.000000 53.000000 0.000000  7.000000 8.000000 1.000000 2.000000 2.000000 0.000000 30.000000  13.000000 12.000000 0.000000 0.000000 208.000000 0.000000 0.000000    1.000000     3.000000 2.000000                   1.000000     '"
          ]
        }
      ],
      "source": [
        "# Logistic Regression 1 on Put Skew Slope - 'Δs_Pdo_o'\n",
        "X1 = balanced_df[['Δs_Pdo_o',\n",
        "                'ATM_IV', 'BidAsk_Spread', 'Volume']] # Because of the model we imported in, x4 represents 'Δs_Pdo_o'/'Δs_Pdo_a' in regression, x1 is 'Volume'\n",
        "y1 = balanced_df['Jump']\n",
        "\n",
        "# Convert Volume column to summed numeric values\n",
        "balanced_df['Volume'] = (\n",
        "    balanced_df['Volume']\n",
        "    .astype(str)  # Ensure we're working with strings\n",
        "    .apply(lambda x: sum(pd.to_numeric(x.split(), errors='coerce')))\n",
        "    .fillna(0)  # Handle cases with invalid/non-numeric values\n",
        ")\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X1_scaled = scaler.fit_transform(X1)\n",
        "X1_scaled = sm.add_constant(X1_scaled)\n",
        "\n",
        "model1 = sm.Logit(y1, X1_scaled).fit()\n",
        "print(\"Logistic Regression on Put Skew Slope - 'Δs_Pdo_o'\")\n",
        "print(model1.summary())\n",
        "\n",
        "# Diagnostic Checks\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Variable\"] = ['const'] + X1.columns.tolist()\n",
        "vif[\"VIF\"] = [variance_inflation_factor(X1_scaled, i) for i in range(X1_scaled.shape[1])]\n",
        "print(\"\\nVariance Inflation Factors:\")\n",
        "print(vif)\n",
        "\n",
        "# Logistic Regression 2 on Put Skew Curvature - 'Δs_Pdo_a'\n",
        "X2 = balanced_df[['Δs_Pdo_a',\n",
        "                'ATM_IV', 'BidAsk_Spread', 'Volume']]\n",
        "y2 = balanced_df['Jump']\n",
        "\n",
        "# Convert Volume column to summed numeric values\n",
        "balanced_df['Volume'] = (\n",
        "    balanced_df['Volume']\n",
        "    .astype(str)  # Ensure we're working with strings\n",
        "    .apply(lambda x: sum(pd.to_numeric(x.split(), errors='coerce')))\n",
        "    .fillna(0)  # Handle cases with invalid/non-numeric values\n",
        ")\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X2_scaled = scaler.fit_transform(X2)\n",
        "X2_scaled = sm.add_constant(X2_scaled)\n",
        "\n",
        "model2 = sm.Logit(y2, X2_scaled).fit()\n",
        "print(model2.summary())\n",
        "\n",
        "# Diagnostic Checks\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Variable\"] = ['const'] + X2.columns.tolist()\n",
        "vif[\"VIF\"] = [variance_inflation_factor(X2_scaled, i) for i in range(X2_scaled.shape[1])]\n",
        "print(\"\\nVariance Inflation Factors:\")\n",
        "print(vif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l31AU-t4xjyT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_SHmzSIrYRk"
      },
      "source": [
        "# Visualizations\n",
        "\n",
        "### 1. Enhanced Coefficient Plot with Significance Stars\n",
        "**What It Shows**:  \n",
        "Horizontal bar plot of logistic regression coefficients with 95% confidence intervals.  \n",
        "- **Bars**: Magnitude and direction of feature effects on log-odds of jumps  \n",
        "- **Colors**: Cool (blue) = negative effects, Warm (red) = positive effects  \n",
        "- **Stars**: Statistical significance (`***` = p&lt;0.001, `**`=p&lt;0.01, `*`=p&lt;0.05)  \n",
        "\n",
        "**Interpretation**:  \n",
        "$$ \\text{log-odds}(\\text{Jump}) = \\beta_0 + \\beta_1 \\Delta s_{Pdo,o} + \\cdots + \\beta_6 \\text{Volume} $$  \n",
        "- *Positive coefficients*: Increase jump probability\n",
        "- *Negative coefficients*: Decrease jump probability\n",
        "---\n",
        "\n",
        "### 2. ROC Curve (AUC = 0.91)  \n",
        "**What It Shows**: Trade-off between true positive rate (TPR) and false positive rate (FPR) across classification thresholds.  \n",
        "\n",
        "**Key Metrics**:  \n",
        "$$ \\text{TPR} = \\frac{\\text{True Positives}}{\\text{Actual Positives}}, \\quad \\text{FPR} = \\frac{\\text{False Positives}}{\\text{Actual Negatives}} $$  \n",
        "- **AUC** = 0.91: Model ranks 91% of jump days higher than non-jump days  \n",
        "- **Diagonal line**: Random guessing (AUC=0.5)  \n",
        "\n",
        "**Interpretation**:  \n",
        "- *Top-left curve*: Ideal performance (high TPR, low FPR)  \n",
        "- *Threshold Selection*: At FPR=10%, TPR≈85% means detecting 85% of jumps while falsely flagging 10% of non-jumps  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwi7DYmJnIf0"
      },
      "outputs": [],
      "source": [
        "# Regression Visualizations - 'Δs_Pdo_o':\n",
        "\n",
        "# Logistic Regression with Enhanced Visualizations\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# 1. Enhanced Coefficient Plot with Significance Stars\n",
        "plt.figure(figsize=(10,6))\n",
        "coefs1 = model1.params[1:]\n",
        "ci1 = model1.conf_int().values[1:]\n",
        "p_values1 = model1.pvalues[1:]\n",
        "\n",
        "# Create significance indicators\n",
        "significance = ['***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else '' for p in p_values1]\n",
        "\n",
        "# Create color gradient based on coefficient magnitude\n",
        "colors = plt.cm.coolwarm(np.linspace(0,1,len(coefs1)))\n",
        "\n",
        "plt.barh(range(len(coefs1)), coefs1, xerr=[coefs1-ci1[:,0], ci1[:,1]-coefs1],\n",
        "        tick_label=[f'{name}\\n{sig}' for name,sig in zip(coefs1.index, significance)],\n",
        "        color=colors, edgecolor='black')\n",
        "\n",
        "plt.axvline(0, color='gray', linestyle='--', alpha=0.7)\n",
        "plt.title(\"Logistic Regression Coefficients with 95% CI\", fontsize=14, pad=20)\n",
        "plt.xlabel(\"Coefficient Magnitude (Log-Odds)\", fontsize=12)\n",
        "plt.ylabel(\"Predictors\", fontsize=12)\n",
        "plt.yticks(fontsize=11)\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. ROC Curve\n",
        "y1_pred_prob = model1.predict(X1_scaled)\n",
        "fpr, tpr, thresholds = roc_curve(y1, y1_pred_prob)\n",
        "auc_score = roc_auc_score(y1, y1_pred_prob)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(fpr, tpr, color='#FF6F61', lw=2, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, pad=15)\n",
        "plt.legend(loc='lower right')\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Predicted Probability Distribution\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.kdeplot(y1_pred_prob[y1 == 0], color='#4C72B0', label='No Jump', fill=True, alpha=0.3)\n",
        "sns.kdeplot(y1_pred_prob[y1 == 1], color='#DD8452', label='Jump', fill=True, alpha=0.3)\n",
        "plt.xlabel('Predicted Probability of Jump', fontsize=12)\n",
        "plt.ylabel('Density', fontsize=12)\n",
        "plt.title('Distribution of Predicted Probabilities', fontsize=14, pad=15)\n",
        "plt.legend()\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Regression Visualizations - 'Δs_Pdo_a':\n",
        "\n",
        "# Logistic Regression with Enhanced Visualizations\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# 1. Enhanced Coefficient Plot with Significance Stars\n",
        "plt.figure(figsize=(10,6))\n",
        "coefs2 = model2.params[1:]\n",
        "ci2 = model2.conf_int().values[1:]\n",
        "p_values2 = model2.pvalues[1:]\n",
        "\n",
        "# Create significance indicators\n",
        "significance = ['***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else '' for p in p_values2]\n",
        "\n",
        "# Create color gradient based on coefficient magnitude\n",
        "colors = plt.cm.coolwarm(np.linspace(0,1,len(coefs2)))\n",
        "\n",
        "plt.barh(range(len(coefs2)), coefs2, xerr=[coefs2-ci2[:,0], ci2[:,1]-coefs2],\n",
        "        tick_label=[f'{name}\\n{sig}' for name,sig in zip(coefs2.index, significance)],\n",
        "        color=colors, edgecolor='black')\n",
        "\n",
        "plt.axvline(0, color='gray', linestyle='--', alpha=0.7)\n",
        "plt.title(\"Logistic Regression Coefficients with 95% CI\", fontsize=14, pad=20)\n",
        "plt.xlabel(\"Coefficient Magnitude (Log-Odds)\", fontsize=12)\n",
        "plt.ylabel(\"Predictors\", fontsize=12)\n",
        "plt.yticks(fontsize=11)\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. ROC Curve\n",
        "y2_pred_prob = model2.predict(X2_scaled)\n",
        "fpr, tpr, thresholds = roc_curve(y2, y2_pred_prob)\n",
        "auc_score = roc_auc_score(y2, y2_pred_prob)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(fpr, tpr, color='#FF6F61', lw=2, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, pad=15)\n",
        "plt.legend(loc='lower right')\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Predicted Probability Distribution\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.kdeplot(y2_pred_prob[y2 == 0], color='#4C72B0', label='No Jump', fill=True, alpha=0.3)\n",
        "sns.kdeplot(y2_pred_prob[y2 == 1], color='#DD8452', label='Jump', fill=True, alpha=0.3)\n",
        "plt.xlabel('Predicted Probability of Jump', fontsize=12)\n",
        "plt.ylabel('Density', fontsize=12)\n",
        "plt.title('Distribution of Predicted Probabilities', fontsize=14, pad=15)\n",
        "plt.legend()\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrU8gdBfgI2_"
      },
      "source": [
        "## Extra: Confirming given IV from dataset using BS\n",
        "\n",
        "To confirm that the data accurately calculates Implied Volatility based on the Black-Scholes formula, we check the values by backsolving it:\n",
        "\n",
        "![B-S_1.gif](data:image/gif;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puJvpi7Wd_py"
      },
      "outputs": [],
      "source": [
        "def norm_cdf(x):\n",
        "  return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0SvrwOADdUY"
      },
      "outputs": [],
      "source": [
        "## Plotting Volatility Smirk\n",
        "def convert_iv_to_numeric(df, iv_col='IV'):\n",
        "    df[iv_col] = (df[iv_col]\n",
        "                  .astype(str)\n",
        "                  .str.replace('%', '')\n",
        "                  .astype(float))\n",
        "    return df\n",
        "\n",
        "def bs_call_price(S, K, T, r, sigma, q=0.0):\n",
        "    if T <= 0:\n",
        "        return max(0.0, S * math.exp(-q*T) - K * math.exp(-r*T))\n",
        "    d1 = (math.log(S/K) + (r - q + 0.5 * sigma**2) * T) / (sigma * math.sqrt(T))\n",
        "    d2 = d1 - sigma * math.sqrt(T)\n",
        "    return S * math.exp(-q*T) * norm_cdf(d1) - K * math.exp(-r*T) * norm_cdf(d2)\n",
        "\n",
        "def bs_put_price(S, K, T, r, sigma, q=0.0):\n",
        "    if T <= 0:\n",
        "        return max(0.0, K * math.exp(-r*T) - S * math.exp(-q*T))\n",
        "    d1 = (math.log(S/K) + (r - q + 0.5 * sigma**2) * T) / (sigma * math.sqrt(T))\n",
        "    d2 = d1 - sigma * math.sqrt(T)\n",
        "    return K * math.exp(-r*T) * norm_cdf(-d2) - S * math.exp(-q*T) * norm_cdf(-d1)\n",
        "\n",
        "def implied_vol(option_type, price, S, K, T, r=0.01, q=0.0, tol=1e-6, max_iter=100):\n",
        "    # Select pricing function\n",
        "    price_func = bs_call_price if option_type.upper() == 'C' else bs_put_price\n",
        "    low, high = 1e-8, 5.0  # vol range 0% to 500%\n",
        "    # Check if target price is in range; if not, return None\n",
        "    if price < price_func(S, K, T, r, low, q) or price > price_func(S, K, T, r, high, q):\n",
        "        return None\n",
        "    for _ in range(max_iter):\n",
        "        mid = 0.5 * (low + high)\n",
        "        mid_price = price_func(S, K, T, r, mid, q)\n",
        "        if abs(mid_price - price) < tol:\n",
        "            return mid\n",
        "        if mid_price < price:\n",
        "            low = mid\n",
        "        else:\n",
        "            high = mid\n",
        "    return mid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EoPhDi-fjso"
      },
      "source": [
        "After defining functions, we calculate IV and compare their respective values against the provided IV (C_IV and P_IV)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTmQVupRaAQM"
      },
      "outputs": [],
      "source": [
        "# For each option, calculate the mid price and then compute IV from our own function.\n",
        "# We then compare our calculated IV with the provided IV (C_IV and P_IV).\n",
        "calc_call_iv = []\n",
        "calc_put_iv = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    S = row['UNDERLYING_LAST']\n",
        "    K = row['STRIKE']\n",
        "    T = row['DTE'] / 252.0  # use trading year (252 days)\n",
        "    # Calculate mid prices for calls and puts\n",
        "    c_mid = 0.5 * (row['C_BID'] + row['C_ASK'])\n",
        "    p_mid = 0.5 * (row['P_BID'] + row['P_ASK'])\n",
        "\n",
        "    # Calculate IV for call and put using our Black-Scholes solver\n",
        "    call_iv_calc = implied_vol('C', c_mid, S, K, T, r=0.01, q=0.0) if c_mid > 0 else np.nan\n",
        "    put_iv_calc = implied_vol('P', p_mid, S, K, T, r=0.01, q=0.0) if p_mid > 0 else np.nan\n",
        "\n",
        "    calc_call_iv.append(call_iv_calc)\n",
        "    calc_put_iv.append(put_iv_calc)\n",
        "\n",
        "# Add our calculated IVs to the DataFrame\n",
        "df['calc_C_IV'] = calc_call_iv\n",
        "df['calc_P_IV'] = calc_put_iv\n",
        "\n",
        "# (Optional) Compare with provided IVs by checking differences\n",
        "df['diff_call_IV'] = df['C_IV'] - df['calc_C_IV']\n",
        "df['diff_put_IV'] = df['P_IV'] - df['calc_P_IV']\n",
        "print(\"Summary of differences (provided - calculated):\")\n",
        "print(df[['diff_call_IV','diff_put_IV']].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNDZErmUyJcM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
